{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adapted from Victor Zhou's CNN published at [this blog post](https://victorzhou.com/blog/keras-cnn-tutorial/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import mnist\n",
    "import numpy as np\n",
    "\n",
    "# Using tf.keras for Ghost Batch Norm capability.\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, BatchNormalization\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "\n",
    "import tensorflow as tf\n",
    "# Disable INFO and WARNING messages from TensorFlow.\n",
    "# Our Keras version (2.2.4 / TF 1.15.0) throws deprecation warnings.\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = mnist.train_images() \n",
    "train_labels = mnist.train_labels()\n",
    "test_images = mnist.test_images()\n",
    "test_labels = mnist.test_labels()\n",
    "\n",
    "# Normalize the images.\n",
    "train_images = (train_images / 255) - 0.5\n",
    "test_images = (test_images / 255) - 0.5\n",
    "\n",
    "# Reshape the images.\n",
    "train_images = np.expand_dims(train_images, axis=3)\n",
    "test_images = np.expand_dims(test_images, axis=3)\n",
    "\n",
    "num_filters = 8\n",
    "filter_size = 3\n",
    "pool_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model.\n",
    "batch_size = 100       # TODO: make this less arbitrary.\n",
    "ghost_batch_size = 10  # TODO: make this less arbitrary.\n",
    "\n",
    "model = Sequential([\n",
    "    # The authors required ghost batch normalization in their experiments.\n",
    "    # The papers they reference use normalization preceding each convolutional layer.\n",
    "    BatchNormalization(virtual_batch_size=ghost_batch_size),\n",
    "    Conv2D(filters=num_filters, kernel_size=filter_size, input_shape=(28, 28, 1)),\n",
    "    MaxPooling2D(pool_size=pool_size),\n",
    "    Flatten(),\n",
    "    Dense(units=10, activation='softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from https://machinelearningmastery.com/using-learning-rate-schedules-deep-learning-models-python-keras/\n",
    "def step_lr_decay(epoch):\n",
    "    # This starts with a learning rate higher than the default learning rate in\n",
    "    # Vanilla SGD. It drops by a factor of 5 each step, and I chose to drop after\n",
    "    # every other epoch to make training not take too long on a laptop.\n",
    "    # Think of this as a half-life equation.\n",
    "    initial_lrate=0.15\n",
    "    drop=0.2\n",
    "    epochs_drop=2.0\n",
    "    current_rate = initial_lrate * math.pow(drop, math.floor(epoch / epochs_drop))\n",
    "    print(\"Epoch %d learning rate: %f\" % (epoch + 1, current_rate))\n",
    "    return current_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model.\n",
    "epochs = 6            # Run enough epochs to test the training schedule.\n",
    "learning_rate = 0.00  # Will be overrided by the scheduler.\n",
    "decay_rate = 0.00     # Default decay rate.\n",
    "momentum = 0.0        # Default momentum.\n",
    "\n",
    "sgd = SGD(lr=learning_rate, momentum=momentum, decay=decay_rate, nesterov=False)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=sgd,  # Vanilla SGD experiment.\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1: Vanilla SGD, decaying learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model.\n",
    "decay_lr_scheduler = LearningRateScheduler(step_lr_decay)\n",
    "\n",
    "exp1_results = model.fit(\n",
    "    train_images,\n",
    "    to_categorical(train_labels),\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=(test_images, to_categorical(test_labels)),\n",
    "    callbacks=[decay_lr_scheduler]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2: Vanilla SGD, increasing batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras has no built in for this. \n",
    "# Code is from https://www.codementor.io/nitinsurya/how-to-re-initialize-keras-model-weights-et41zre2g\n",
    "def reset_weights(model):\n",
    "    session = K.get_session()\n",
    "    for layer in model.layers: \n",
    "        if hasattr(layer, 'kernel_initializer'):\n",
    "            layer.kernel.initializer.run(session=session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_weights(model)\n",
    "learning_rate = 0.07  # Set a new constant.\n",
    "sgd = SGD(lr=learning_rate, momentum=momentum, decay=decay_rate, nesterov=False)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=sgd,  # Vanilla SGD experiment.\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "# Just run each step independently. Keras has no callback for this.\n",
    "model.fit(\n",
    "    train_images,\n",
    "    to_categorical(train_labels),\n",
    "    epochs=2,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=(test_images, to_categorical(test_labels))\n",
    ")\n",
    "model.fit(\n",
    "    train_images,\n",
    "    to_categorical(train_labels),\n",
    "    epochs=2,\n",
    "    batch_size=batch_size * 5,\n",
    "    validation_data=(test_images, to_categorical(test_labels))\n",
    ")\n",
    "model.fit(\n",
    "    train_images,\n",
    "    to_categorical(train_labels),\n",
    "    epochs=2,\n",
    "    batch_size=batch_size * 25,\n",
    "    validation_data=(test_images, to_categorical(test_labels))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 3: Vanilla SGD, hybrid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_lr_decay(epoch):\n",
    "    # Start off from the constant learning rate from the hybrid's initialization.\n",
    "    initial_lrate=0.05\n",
    "    drop=0.2\n",
    "    epochs_drop=2.0\n",
    "    current_rate = initial_lrate * math.pow(drop, math.floor(epoch / epochs_drop))\n",
    "    print(\"Epoch %d learning rate: %f\" % (epoch + 1, current_rate))\n",
    "    return current_rate\n",
    "\n",
    "hybrid_lr_scheduler = LearningRateScheduler(hybrid_lr_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_weights(model)\n",
    "learning_rate = 0.05  # Set a new constant.\n",
    "sgd = SGD(lr=learning_rate, momentum=momentum, decay=decay_rate, nesterov=False)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=sgd,  # Vanilla SGD experiment.\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "# Run a step, and then increase the batch size manually. The decay scheduler\n",
    "# will decay the learning rate in the next step.\n",
    "model.fit(\n",
    "    train_images,\n",
    "    to_categorical(train_labels),\n",
    "    epochs=2,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=(test_images, to_categorical(test_labels))\n",
    ")\n",
    "model.fit(\n",
    "    train_images,\n",
    "    to_categorical(train_labels),\n",
    "    epochs=4,\n",
    "    batch_size=batch_size * 5,\n",
    "    validation_data=(test_images, to_categorical(test_labels)),\n",
    "    callbacks=[hybrid_lr_scheduler]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 4: SGD with momentum, decaying learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
