{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adapted from Victor Zhou's CNN published at [this blog post](https://victorzhou.com/blog/keras-cnn-tutorial/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import mnist\n",
    "import numpy as np\n",
    "\n",
    "# Using tf.keras for Ghost Batch Norm capability.\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, BatchNormalization\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras.optimizers import Nadam\n",
    "\n",
    "import tensorflow as tf\n",
    "#print(tf.version)\n",
    "# Disable INFO and WARNING messages from TensorFlow.\n",
    "# Our Keras version (2.2.4 / TF 1.15.0) throws deprecation warnings.\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = mnist.train_images() \n",
    "train_labels = mnist.train_labels()\n",
    "test_images = mnist.test_images()\n",
    "test_labels = mnist.test_labels()\n",
    "\n",
    "# Normalize the images.\n",
    "train_images = (train_images / 255) - 0.5\n",
    "test_images = (test_images / 255) - 0.5\n",
    "\n",
    "# Reshape the images.\n",
    "train_images = np.expand_dims(train_images, axis=3)\n",
    "test_images = np.expand_dims(test_images, axis=3)\n",
    "\n",
    "num_filters = 8\n",
    "filter_size = 3\n",
    "pool_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model.\n",
    "batch_size = 100       # TODO: make this less arbitrary.\n",
    "ghost_batch_size = 10  # TODO: make this less arbitrary.\n",
    "\n",
    "model = Sequential([\n",
    "    # The authors required ghost batch normalization in their experiments.\n",
    "    # The papers they reference use normalization preceding each convolutional layer.\n",
    "    BatchNormalization(virtual_batch_size=ghost_batch_size),\n",
    "    Conv2D(filters=num_filters, kernel_size=filter_size, input_shape=(28, 28, 1)),\n",
    "    MaxPooling2D(pool_size=pool_size),\n",
    "    Flatten(),\n",
    "    Dense(units=10, activation='softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from https://machinelearningmastery.com/using-learning-rate-schedules-deep-learning-models-python-keras/\n",
    "def step_lr_decay(epoch):\n",
    "    # This starts with a learning rate higher than the default learning rate in\n",
    "    # Vanilla SGD. It drops by a factor of 5 each step, and I chose to drop after\n",
    "    # every other epoch to make training not take too long on a laptop.\n",
    "    # Think of this as a half-life equation.\n",
    "    initial_lrate=0.15\n",
    "    drop=0.2\n",
    "    epochs_drop=2.0\n",
    "    current_rate = initial_lrate * math.pow(drop, math.floor(epoch / epochs_drop))\n",
    "    print(\"Epoch %d learning rate: %f\" % (epoch + 1, current_rate))\n",
    "    return current_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model.\n",
    "epochs = 6            # Run enough epochs to test the training schedule.\n",
    "learning_rate = 0.00  # Will be overrided by the scheduler.\n",
    "decay_rate = 0.00     # Default decay rate.\n",
    "momentum = 0.0        # Default momentum.\n",
    "\n",
    "sgd = SGD(lr=learning_rate, momentum=momentum, decay=decay_rate, nesterov=False)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=sgd,  # Vanilla SGD experiment.\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1: Vanilla SGD, decaying learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1 learning rate: 0.150000\n",
      "Epoch 1/6\n",
      "60000/60000 [==============================] - 6s 95us/sample - loss: 0.2490 - acc: 0.9255 - val_loss: 0.1272 - val_acc: 0.9601\n",
      "Epoch 2 learning rate: 0.150000\n",
      "Epoch 2/6\n",
      "60000/60000 [==============================] - 6s 95us/sample - loss: 0.1177 - acc: 0.9642 - val_loss: 0.1123 - val_acc: 0.9644\n",
      "Epoch 3 learning rate: 0.030000\n",
      "Epoch 3/6\n",
      "60000/60000 [==============================] - 6s 94us/sample - loss: 0.0734 - acc: 0.9790 - val_loss: 0.0746 - val_acc: 0.9774\n",
      "Epoch 4 learning rate: 0.030000\n",
      "Epoch 4/6\n",
      "60000/60000 [==============================] - 6s 94us/sample - loss: 0.0654 - acc: 0.9807 - val_loss: 0.0749 - val_acc: 0.9763\n",
      "Epoch 5 learning rate: 0.006000\n",
      "Epoch 5/6\n",
      "60000/60000 [==============================] - 6s 95us/sample - loss: 0.0577 - acc: 0.9833 - val_loss: 0.0713 - val_acc: 0.9775\n",
      "Epoch 6 learning rate: 0.006000\n",
      "Epoch 6/6\n",
      "60000/60000 [==============================] - 6s 93us/sample - loss: 0.0562 - acc: 0.9836 - val_loss: 0.0716 - val_acc: 0.9778\n"
     ]
    }
   ],
   "source": [
    "# Train the model.\n",
    "decay_lr_scheduler = LearningRateScheduler(step_lr_decay)\n",
    "\n",
    "exp1_results = model.fit(\n",
    "    train_images,\n",
    "    to_categorical(train_labels),\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=(test_images, to_categorical(test_labels)),\n",
    "    callbacks=[decay_lr_scheduler]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2: Vanilla SGD, increasing batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras has no built in for this. \n",
    "# Code is from https://www.codementor.io/nitinsurya/how-to-re-initialize-keras-model-weights-et41zre2g\n",
    "def reset_weights(model):\n",
    "    session = K.get_session()\n",
    "    for layer in model.layers: \n",
    "        if hasattr(layer, 'kernel_initializer'):\n",
    "            layer.kernel.initializer.run(session=session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "60000/60000 [==============================] - 6s 96us/sample - loss: 0.2630 - acc: 0.9213 - val_loss: 0.1394 - val_acc: 0.9582\n",
      "Epoch 2/2\n",
      "60000/60000 [==============================] - 6s 95us/sample - loss: 0.1217 - acc: 0.9646 - val_loss: 0.1083 - val_acc: 0.9657\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "60000/60000 [==============================] - 5s 87us/sample - loss: 0.0821 - acc: 0.9761 - val_loss: 0.0840 - val_acc: 0.9739\n",
      "Epoch 2/2\n",
      "60000/60000 [==============================] - 5s 87us/sample - loss: 0.0753 - acc: 0.9785 - val_loss: 0.0823 - val_acc: 0.9735\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "60000/60000 [==============================] - 5s 78us/sample - loss: 0.0687 - acc: 0.9809 - val_loss: 0.0798 - val_acc: 0.9745\n",
      "Epoch 2/2\n",
      "60000/60000 [==============================] - 5s 78us/sample - loss: 0.0674 - acc: 0.9812 - val_loss: 0.0797 - val_acc: 0.9752\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x183fc808>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reset_weights(model)\n",
    "learning_rate = 0.07  # Set a new constant.\n",
    "sgd = SGD(lr=learning_rate, momentum=momentum, decay=decay_rate, nesterov=False)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=sgd,  # Vanilla SGD experiment.\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "# Just run each step independently. Keras has no callback for this.\n",
    "model.fit(\n",
    "    train_images,\n",
    "    to_categorical(train_labels),\n",
    "    epochs=2,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=(test_images, to_categorical(test_labels))\n",
    ")\n",
    "model.fit(\n",
    "    train_images,\n",
    "    to_categorical(train_labels),\n",
    "    epochs=2,\n",
    "    batch_size=batch_size * 5,\n",
    "    validation_data=(test_images, to_categorical(test_labels))\n",
    ")\n",
    "model.fit(\n",
    "    train_images,\n",
    "    to_categorical(train_labels),\n",
    "    epochs=2,\n",
    "    batch_size=batch_size * 25,\n",
    "    validation_data=(test_images, to_categorical(test_labels))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 3: Vanilla SGD, hybrid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_lr_decay(epoch):\n",
    "    # Start off from the constant learning rate from the hybrid's initialization.\n",
    "    initial_lrate=0.05\n",
    "    drop=0.2\n",
    "    epochs_drop=2.0\n",
    "    current_rate = initial_lrate * math.pow(drop, math.floor(epoch / epochs_drop))\n",
    "    print(\"Epoch %d learning rate: %f\" % (epoch + 1, current_rate))\n",
    "    return current_rate\n",
    "\n",
    "hybrid_lr_scheduler = LearningRateScheduler(hybrid_lr_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "60000/60000 [==============================] - 6s 93us/sample - loss: 0.2621 - acc: 0.9235 - val_loss: 0.1302 - val_acc: 0.9624\n",
      "Epoch 2/2\n",
      "60000/60000 [==============================] - 6s 98us/sample - loss: 0.1180 - acc: 0.9658 - val_loss: 0.1001 - val_acc: 0.9687\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1 learning rate: 0.050000\n",
      "Epoch 1/4\n",
      "60000/60000 [==============================] - 5s 89us/sample - loss: 0.0807 - acc: 0.9767 - val_loss: 0.0800 - val_acc: 0.9764\n",
      "Epoch 2 learning rate: 0.050000\n",
      "Epoch 2/4\n",
      "60000/60000 [==============================] - 6s 95us/sample - loss: 0.0755 - acc: 0.9782 - val_loss: 0.0789 - val_acc: 0.9768\n",
      "Epoch 3 learning rate: 0.010000\n",
      "Epoch 3/4\n",
      "60000/60000 [==============================] - 6s 92us/sample - loss: 0.0696 - acc: 0.9801 - val_loss: 0.0772 - val_acc: 0.9775\n",
      "Epoch 4 learning rate: 0.010000\n",
      "Epoch 4/4\n",
      "60000/60000 [==============================] - 5s 90us/sample - loss: 0.0687 - acc: 0.9809 - val_loss: 0.0772 - val_acc: 0.9769\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x18479208>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reset_weights(model)\n",
    "learning_rate = 0.05  # Set a new constant.\n",
    "sgd = SGD(lr=learning_rate, momentum=momentum, decay=decay_rate, nesterov=False)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=sgd,  # Vanilla SGD experiment.\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "# Run a step, and then increase the batch size manually. The decay scheduler\n",
    "# will decay the learning rate in the next step.\n",
    "model.fit(\n",
    "    train_images,\n",
    "    to_categorical(train_labels),\n",
    "    epochs=2,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=(test_images, to_categorical(test_labels))\n",
    ")\n",
    "model.fit(\n",
    "    train_images,\n",
    "    to_categorical(train_labels),\n",
    "    epochs=4,\n",
    "    batch_size=batch_size * 5,\n",
    "    validation_data=(test_images, to_categorical(test_labels)),\n",
    "    callbacks=[hybrid_lr_scheduler]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 4: Adam, decaying learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1 learning rate: 0.150000\n",
      "Epoch 1/6\n",
      "60000/60000 [==============================] - 6s 99us/sample - loss: 3.1535 - acc: 0.8614 - val_loss: 0.2747 - val_acc: 0.9241\n",
      "Epoch 2 learning rate: 0.150000\n",
      "Epoch 2/6\n",
      "60000/60000 [==============================] - 6s 96us/sample - loss: 0.2484 - acc: 0.9293 - val_loss: 0.1718 - val_acc: 0.9504\n",
      "Epoch 3 learning rate: 0.030000\n",
      "Epoch 3/6\n",
      "60000/60000 [==============================] - 6s 97us/sample - loss: 0.1397 - acc: 0.9605 - val_loss: 0.1351 - val_acc: 0.9609\n",
      "Epoch 4 learning rate: 0.030000\n",
      "Epoch 4/6\n",
      "60000/60000 [==============================] - 6s 97us/sample - loss: 0.1185 - acc: 0.9656 - val_loss: 0.1149 - val_acc: 0.9660\n",
      "Epoch 5 learning rate: 0.006000\n",
      "Epoch 5/6\n",
      "60000/60000 [==============================] - 6s 96us/sample - loss: 0.0931 - acc: 0.9731 - val_loss: 0.1052 - val_acc: 0.9691\n",
      "Epoch 6 learning rate: 0.006000\n",
      "Epoch 6/6\n",
      "60000/60000 [==============================] - 6s 97us/sample - loss: 0.0885 - acc: 0.9747 - val_loss: 0.0991 - val_acc: 0.9691\n"
     ]
    }
   ],
   "source": [
    "reset_weights(model)\n",
    "nadam = Nadam() #The paper uses the default settings for Adam\n",
    "\n",
    "model.compile(\n",
    "    optimizer=nadam,  # Adam experiment.\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "# Train the model.\n",
    "decay_lr_scheduler = LearningRateScheduler(step_lr_decay)\n",
    "\n",
    "exp4_results = model.fit(\n",
    "    train_images,\n",
    "    to_categorical(train_labels),\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=(test_images, to_categorical(test_labels)),\n",
    "    callbacks=[decay_lr_scheduler]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 5: Adam, increasing batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "60000/60000 [==============================] - 6s 101us/sample - loss: 0.5175 - acc: 0.8552 - val_loss: 0.2535 - val_acc: 0.9270\n",
      "Epoch 2/2\n",
      "60000/60000 [==============================] - 6s 96us/sample - loss: 0.2282 - acc: 0.9344 - val_loss: 0.1831 - val_acc: 0.9488\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "60000/60000 [==============================] - 5s 88us/sample - loss: 0.1749 - acc: 0.9512 - val_loss: 0.1677 - val_acc: 0.9545\n",
      "Epoch 2/2\n",
      "60000/60000 [==============================] - 5s 89us/sample - loss: 0.1626 - acc: 0.9548 - val_loss: 0.1592 - val_acc: 0.9563\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "60000/60000 [==============================] - 5s 80us/sample - loss: 0.1530 - acc: 0.9566 - val_loss: 0.1553 - val_acc: 0.9570\n",
      "Epoch 2/2\n",
      "60000/60000 [==============================] - 5s 78us/sample - loss: 0.1510 - acc: 0.9581 - val_loss: 0.1532 - val_acc: 0.9577\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1b9fb948>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reset_weights(model)\n",
    "nadam = Nadam() #The paper uses the default settings for Adam\n",
    "\n",
    "model.compile(\n",
    "    optimizer=nadam,  # Adam experiment.\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "# Just run each step independently. Keras has no callback for this.\n",
    "model.fit(\n",
    "    train_images,\n",
    "    to_categorical(train_labels),\n",
    "    epochs=2,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=(test_images, to_categorical(test_labels))\n",
    ")\n",
    "model.fit(\n",
    "    train_images,\n",
    "    to_categorical(train_labels),\n",
    "    epochs=2,\n",
    "    batch_size=batch_size * 5,\n",
    "    validation_data=(test_images, to_categorical(test_labels))\n",
    ")\n",
    "model.fit(\n",
    "    train_images,\n",
    "    to_categorical(train_labels),\n",
    "    epochs=2,\n",
    "    batch_size=batch_size * 25,\n",
    "    validation_data=(test_images, to_categorical(test_labels))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 6: Adam, hybrid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_lr_decay_adam(epoch):\n",
    "    # Start off from the constant learning rate from the hybrid's initialization.\n",
    "    initial_lrate=0.001 #initial learning rate for Adam's Keras's default = 0.001\n",
    "    drop=0.2\n",
    "    epochs_drop=2.0\n",
    "    current_rate = initial_lrate * math.pow(drop, math.floor(epoch / epochs_drop))\n",
    "    print(\"Epoch %d learning rate: %f\" % (epoch + 1, current_rate))\n",
    "    return current_rate\n",
    "\n",
    "hybrid_lr_scheduler_adam = LearningRateScheduler(hybrid_lr_decay_adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "60000/60000 [==============================] - 6s 104us/sample - loss: 0.4084 - acc: 0.8821 - val_loss: 0.2391 - val_acc: 0.9303\n",
      "Epoch 2/2\n",
      "60000/60000 [==============================] - 6s 100us/sample - loss: 0.2087 - acc: 0.9410 - val_loss: 0.1620 - val_acc: 0.9548\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1 learning rate: 0.001000\n",
      "Epoch 1/4\n",
      "60000/60000 [==============================] - 5s 90us/sample - loss: 0.1518 - acc: 0.9576 - val_loss: 0.1431 - val_acc: 0.9579\n",
      "Epoch 2 learning rate: 0.001000\n",
      "Epoch 2/4\n",
      "60000/60000 [==============================] - 5s 90us/sample - loss: 0.1396 - acc: 0.9606 - val_loss: 0.1327 - val_acc: 0.9608\n",
      "Epoch 3 learning rate: 0.000200\n",
      "Epoch 3/4\n",
      "60000/60000 [==============================] - 5s 90us/sample - loss: 0.1303 - acc: 0.9637 - val_loss: 0.1311 - val_acc: 0.9613\n",
      "Epoch 4 learning rate: 0.000200\n",
      "Epoch 4/4\n",
      "60000/60000 [==============================] - 5s 90us/sample - loss: 0.1288 - acc: 0.9640 - val_loss: 0.1289 - val_acc: 0.9616\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1cdef6c8>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reset_weights(model)\n",
    "nadam = Nadam() #The paper uses the default settings for Adam\n",
    "\n",
    "model.compile(\n",
    "    optimizer=nadam,  # Adam experiment.\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "# Run a step, and then increase the batch size manually. The decay scheduler\n",
    "# will decay the learning rate in the next step.\n",
    "model.fit(\n",
    "    train_images,\n",
    "    to_categorical(train_labels),\n",
    "    epochs=2,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=(test_images, to_categorical(test_labels))\n",
    ")\n",
    "model.fit(\n",
    "    train_images,\n",
    "    to_categorical(train_labels),\n",
    "    epochs=4,\n",
    "    batch_size=batch_size * 5,\n",
    "    validation_data=(test_images, to_categorical(test_labels)),\n",
    "    callbacks=[hybrid_lr_scheduler_adam]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
